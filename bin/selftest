#!/usr/bin/env bash
#
# Copyright 2011-2018, Marshall McMullen <marshall.mcmullen@gmail.com>
# Copyright 2011-2018, SolidFire, Inc. All rights reserved.
#
# This program is free software: you can redistribute it and/or modify it under the terms of the Apache License
# as published by the Apache Software Foundation, either version 2 of the License, or (at your option) any later
# version.

#----------------------------------------------------------------------------------------------------------------------
#
# Setup
#
#----------------------------------------------------------------------------------------------------------------------

# Source ebash from the same directory as this script
$( $(dirname $0)/ebash --source )

# Some ebash recognized environment variables interfer with functionality of the selftests so we need to unset them.
unset BREAK
unset EDEBUG
unset EFUNCS_COLOR
unset EINTERACTIVE
unset FAILURES
unset FILTER
unset REPEAT
unset REPEAT
unset VERBOSE

#----------------------------------------------------------------------------------------------------------------------
#
# Test Helpers
#
#----------------------------------------------------------------------------------------------------------------------

# All of the asserts below depend on etest having been run by this function, which sets up the values of ETEST_OUTPUT,
# ETEST_LOG, and ETEST_RC.
#
declare ETEST_OUTPUT="" ETEST_LOG="" ETEST_RC=255
run_etest()
{
    ebanner "$(caller 0 | awk '{print $2}')"

    local rc=0
    EDEBUG="" ${ETEST} --no-subreaper "${@}" |& tee output.log && rc=0 || rc=1
    ETEST_RC=${rc}
    ETEST_OUTPUT=$(< output.log)
    ETEST_LOG=$(< etest.log)
    ETEST_JSON="etest.json"
    rm -f output.log etest.log
}

# Accepts a list of test names and asserts that the test was properly reported as a failing test. Requires that you run
# etest via run_etest.
#
# WARNING: Won't deal well when facing two or more tests where the name of one test is simply an addition of characters
# to the end of the other test name. Just don't do that in the tests or fix these asserts
assert_test_fail()
{
    local this_test
    for this_test in "${@}" ; do

        # If you're not paying attention to ansi codes that move your cursor, it looks like the test name and its
        # result are on two separate lines. Grab both and make sure it's listed as OK. We'll catch errors in the
        # assert call, so ignore them for the initial grab
        output_state="$(echo "${ETEST_OUTPUT}" | grep -PA1 "${this_test}\b" || true)"
        assert_match "${output_state}" "${this_test}"
        assert_match "${output_state}" " !! "

        # Make sure the test was listed as FAILED in the log and that it's not in the lists of failed or flaky tests
        assert_match "${ETEST_LOG}" "${this_test} FAILED"
        assert_not_match "${ETEST_OUTPUT}" "FAILED TESTS:.*${this_test}"
        assert_not_match "${ETEST_LOG}" "FAILED TESTS:.*${this_test}"
        assert_not_match "${ETEST_OUTPUT}" "FLAKY TESTS:.*${this_test}"
        assert_not_match "${ETEST_LOG}" "FLAKY TESTS:.*${this_test}"

        # Also validate the test is listed in the list of failing tests in the json output file
        jq --raw-output '.testsFailed | .[]' "${ETEST_JSON}" | grep --quiet "${this_test}"
    done

    # Verify count of failing tests
    assert_eq "$#" "$(jq --raw-output .numTestsFailed "${ETEST_JSON}")"
}

# Accepts a list of test names and asserts that the test was properly reported as a passing test. Requires that you run
# etest via run_etest.
#
# WARNING: Won't deal well when facing two or more tests where the name of one test is simply an addition of characters
# to the end of the other test name. Just don't do that in the tests or fix these asserts
assert_test_pass()
{
    local this_test
    for this_test in "${@}" ; do

        # If you're not paying attention to ansi codes that move your cursor, it looks like the test name and its
        # result are on two separate lines. Grab both and make sure it's listed as OK. We'll catch errors in the
        # assert call, so ignore them for the initial grab
        output_state="$(echo "${ETEST_OUTPUT}" | grep -PA1 "${this_test}\b" || true)"
        echo "${output_state}" | grep -q "ok" || true
        assert_match "${output_state}" "${this_test}"
        assert_match "${output_state}" " ok "

        # Make sure the test was listed as passed in the log, too
        assert_match "${ETEST_LOG}" "${this_test} PASSED"

        # And make sure it's not in either of the lists of failed tests
        assert_not_match "${ETEST_OUTPUT}" "FAILED TESTS:.*${this_test}"
        assert_not_match "${ETEST_LOG}" "FAILED TESTS:.*${this_test}"
        assert_not_match "${ETEST_OUTPUT}" "FLAKY TESTS:.*${this_test}"
        assert_not_match "${ETEST_LOG}" "FLAKY TESTS:.*${this_test}"

        # Ensure the test is reported in the list of passing tests in the json output file
        jq --raw-output '.testsPassed | .[]' "${ETEST_JSON}" | grep --quiet "${this_test}"
    done

    # Verify count of passing tests
    assert_eq "$#" "$(jq --raw-output .numTestsPassed "${ETEST_JSON}")"
}

# Accepts a list of test names and asserts that the test was properly reported as a flaky test. Requires that you run
# etest via run_etest.
#
# WARNING: Won't deal well when facing two or more tests where the name of one test is simply an addition of characters
# to the end of the other test name. Just don't do that in the tests or fix these asserts
assert_test_flaky()
{
    local this_test
    for this_test in "${@}" ; do

        # If you're not paying attention to ansi codes that move your cursor, it looks like the test name and its
        # result are on two separate lines. Grab both and make sure it's listed as OK. We'll catch errors in the
        # assert call, so ignore them for the initial grab
        output_state="$(echo "${ETEST_OUTPUT}" | grep -PA1 "${this_test}\b" || true)"
        echo "${output_state}" | grep -q "ok" || true
        assert_match "${output_state}" "${this_test}"
        assert_match "${output_state}" " ok "

        # Make sure the test was listed as passed in the log, too
        assert_match "${ETEST_LOG}" "${this_test} PASSED"

        # And make sure it's not in either of the lists of failed tests
        assert_not_match "${ETEST_OUTPUT}" "FAILED TESTS:.*${this_test}"
        assert_not_match "${ETEST_LOG}" "FAILED TESTS:.*${this_test}"

        # And make sure it was reported as a flaky test
        assert_match "${ETEST_OUTPUT}" "FLAKY TESTS:.*${this_test}"
        assert_match "${ETEST_LOG}" "FLAKY TESTS:.*${this_test}"

        # Ensure the test is reported in the list of passing and flaky tests in the json output file
        jq --raw-output '.testsPassed | .[]' "${ETEST_JSON}" | grep --quiet "${this_test}"
        jq --raw-output '.testsFlaky  | .[]' "${ETEST_JSON}" | grep --quiet "${this_test}"
    done

    # Verify count of flaky tests
    assert_eq "$#" "$(jq --raw-output .numTestsFlaky  "${ETEST_JSON}")"
}

# Verifies that output and log looks correct for a specified number of tests attempted and passed. For instance, if you
# call assert_test_count 2 2, that should mean that 2 tests were run and both passed. It also looks at the new json
# output file and makes sure the numbers are correct.
#
# You must call etest by the run_etest function to use this assert.
#
assert_test_count()
{
    $(opt_parse \
        ":pass  | Number of tests expected to pass." \
        ":fail  | Number of tests expected to fail." \
        ":flaky | Number of tests expected to be flaky." \
        ":total | Total number of tests expected to be executed.")

    argcheck pass fail flaky total

    assert_match "${ETEST_OUTPUT}" "${pass}/${total} tests passed"
    assert_match "${ETEST_LOG}"    "${pass}/${total} tests passed"

    if [[ pass -lt total ]] ; then
        assert_not_zero "${ETEST_RC}"
    else
        assert_zero "${ETEST_RC}"
    fi

    # Verify count of all tests in json file
    assert_eq "${pass}"  "$(jq --raw-output .numTestsPassed   "${ETEST_JSON}")"
    assert_eq "${fail}"  "$(jq --raw-output .numTestsFailed   "${ETEST_JSON}")"
    assert_eq "${flaky}" "$(jq --raw-output .numTestsFlaky    "${ETEST_JSON}")"
    assert_eq "${total}" "$(jq --raw-output .numTestsExecuted "${ETEST_JSON}")"
}

#----------------------------------------------------------------------------------------------------------------------
#
# Tests
#
#----------------------------------------------------------------------------------------------------------------------

# Verify setup/teardown correctness:
# - suite_setup called once
# - suite_teardown called once
# - setup called for each test
# - teardown called for each test
SELFTEST_assert_setup_teardown()
{
    local output="setup_teardown.out"
    > "${output}"
    trap_add "rm ${output}"

    run_etest setup_teardown.etest
    assert_test_count --pass 3 --fail 0 --flaky 0 --total 3
    assert_exists "${output}"

    expected=$(cat <<- 'END'
	suite_setup
	setup
	1
	teardown
	setup
	2
	teardown
	setup
	3
	teardown
	suite_teardown
	END
    )

    assert_eq "${expected}" "$(cat ${output})"
}

# Just passing standalone tests
SELFTEST_simple()
{
    run_etest plain_script sources_ebash noisy
    assert_test_count --pass 3 --fail 0 --flaky 0 --total 3
    assert_test_pass plain_script sources_ebash noisy
    assert_not_match "${ETEST_OUTPUT}" "!!"
    assert_match "${ETEST_LOG}" VERY_NOISY_STDOUT
    assert_match "${ETEST_LOG}" VERY_NOISY_STDERR
}

# Passing and failing standalone tests
SELFTEST_passing_failing_standalone()
{
    run_etest fail_standalone plain_script sources_ebash noisy
    assert_test_count --pass 3 --fail 1 --flaky 0 --total 4
    assert_test_pass plain_script sources_ebash noisy
    assert_test_fail fail_standalone
}

# Mixed passing and failing etests
SELFTEST_passing_and_failing_etests()
{
    run_etest pass_many.etest fail_many.etest
    assert_test_count --pass 7 --fail 7 --flaky 0 --total 14
    assert_test_pass A{1..4} B{2..3} pass_noisy
    assert_test_fail A{5..7} B{4..6} fail_noisy
}

# Filter to get just the passing etests
SELFTEST_filter_passing()
{
    run_etest --filter "(A[1234]|B[23]|pass)" pass_many.etest fail_many.etest
    assert_test_count --pass 7 --fail 0 --flaky 0 --total 7
    assert_test_pass A{1..4} B{2..3} pass_noisy
}

# Exclude and get just the failing etests
SELFTEST_exclude_passing()
{
    run_etest --exclude "(A[1234]|B[23]|pass)" pass_many.etest fail_many.etest
    assert_test_count --pass 0 --fail 7 --flaky 0 --total 7
    assert_test_fail A{5..7} B{4..6} fail_noisy
}

# Make sure filter can be passed as an environment variable (PE-2332)
SELFTEST_filter_env_variable()
{
    export FILTER='(A[1234]|B[23]|pass)'
    run_etest pass_many.etest fail_many.etest
    assert_test_count --pass 7 --fail 0 --flaky 0 --total 7
    assert_test_pass A{1..4} B{2..3} pass_noisy
}

# Make sure exclude can be passed as an environment variable (PE-2332)
SELFTEST_exclude_env_variable()
{
    export EXCLUDE='(A[1234]|B[23]|pass)'
    run_etest pass_many.etest fail_many.etest
    assert_test_count --pass 0 --fail 7 --flaky 0 --total 7
    assert_test_fail A{5..7} B{4..6} fail_noisy
}

# Test passes second time failures not allowed (default)
SELFTEST_flaky_failures_default()
{
    run_etest --filter flaky_fails_once flaky.etest
    assert_test_count --pass 0 --fail 1 --flaky 0 --total 1
    assert_test_fail flaky_fails_once
}

# Test passes second time with failures=0
SELFTEST_flaky_failures_0()
{
    run_etest --filter flaky_fails_once --failures 0 flaky.etest
    assert_test_count --pass 0 --fail 1 --flaky 0 --total 1
    assert_test_fail flaky_fails_once
}

# Test passes second time with failures=1
SELFTEST_flaky_failures_1()
{
    run_etest --filter flaky_fails_once --failures 1 flaky.etest
    assert_test_count --pass 1 --fail 0 --flaky 1 --total 1
    assert_test_pass flaky_fails_once
    assert_test_flaky flaky_fails_once
}

# Test passes third time failures=1
SELFTEST_flaky_failures_2()
{
    run_etest --filter flaky_fails_twice --failures 1 flaky.etest
    assert_test_count --pass 0 --fail 1 --flaky 0 --total 1
    assert_test_fail flaky_fails_twice
}

# Flkay test with a test that passes third time and failures=3
SELFTEST_flaky_failures_3()
{
    run_etest --filter flaky_fails_twice --failures 3 flaky.etest
    assert_test_count --pass 1 --fail 0 --flaky 1 --total 1
    assert_test_pass flaky_fails_twice
    assert_test_flaky flaky_fails_twice
}

# Flaky test. Passing, failing and flaky tests with failures=1
SELFTEST_flaky_failures_mixed()
{
    run_etest --failures 1 pass_many.etest fail_many.etest flaky.etest
    assert_test_count --pass 8 --fail 8 --flaky 1 --total 16
    assert_test_pass A{1..4} B{2..3} pass_noisy flaky_fails_once
    assert_test_fail A{5..7} B{4..6} fail_noisy flaky_fails_twice
    assert_test_flaky flaky_fails_once
}

SELFTEST_hang_standalone()
{
    run_etest --timeout 5s hang_standalone
    assert_test_count --pass 0 --fail 1 --flaky 0 --total 1
    assert_test_fail hang_standalone
}

SELFTEST_hang_total_timeout()
{
    run_etest --timeout 30s hang.etest
    assert_test_count --pass 2 --fail 1 --flaky 0 --total 3
    assert_test_pass hang_short hang_long
    assert_test_fail hang_infinity
}

SELFTEST_hang_override()
{
    start=${SECONDS}

    # Set a very large timeout. The test overrides this with 5s. So make sure it doesn't hang for the full 5m.
    run_etest --timeout 5m "hang_override.etest"
    assert_test_count --pass 0 --fail 1 --flaky 0 --total 1
    assert_test_fail hang_infinity

    runtime=$(( SECONDS - start ))

    assert_lt "${runtime}" 30
}

#----------------------------------------------------------------------------------------------------------------------
#
# MAIN
#
#----------------------------------------------------------------------------------------------------------------------

ETEST=${EBASH_HOME}/bin/etest
[[ -x ${ETEST} ]] || die "Unable to find etest $(lval ETEST EBASH EBASH_HOME)"
cd ${EBASH_HOME}/tests/self

SELFTESTS=( $(declare -F | awk '$3 ~ "^SELFTEST_" {print $3}' || true) )
array_sort SELFTESTS

# Run each selftest in its own subshell. Also create a output directory for the selftest to use that is persistent
# outside of etest's normal automatic directory deletion that happens after a test run is complete. This allows a
# test to have a persistent place to store things. This is necessary for our flaky tests where we need to keep track
# of how many times the test has been executed so we need a persistent place to store that information.
for selftest in "${SELFTESTS[@]}"; do
(
    declare -xg SELFTEST_DIR_OUTPUT
    SELFTEST_DIR_OUTPUT="$(readlink -f output)"
    efreshdir "${SELFTEST_DIR_OUTPUT}"

    "${selftest}"

    rm -rf "${SELFTEST_DIR_OUTPUT}"
)
done

ebanner "Etest passed all self tests."
