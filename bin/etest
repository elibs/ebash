#!/usr/bin/env bash
#
# Copyright 2011-2018, Marshall McMullen <marshall.mcmullen@gmail.com>
# Copyright 2011-2018, SolidFire, Inc. All rights reserved.
#
# This program is free software: you can redistribute it and/or modify it under the terms of the Apache License
# as published by the Apache Software Foundation, either version 2 of the License, or (at your option) any later
# version.

: ${EMSG_PREFIX:=time}

: ${EBASH_HOME:=$(dirname $0)/..}
: ${EBASH:=${EBASH_HOME}/share}
source ${EBASH}/ebash.sh || { echo "Unable to source ebash." ; exit 1 ; }
export EBASH
export PATH="${PATH}:${EBASH_HOME}/bin"

if [[ ${__EBASH_OS} == "Linux" ]] ; then
    reexec --sudo
fi

# Normalize EBASH path in case any tests depend on it looking... normal.
# Note: wait until after sourcing so that we can let ebash make sure we get
# GNU readlink rater than BSD readlink.
EBASH_HOME=$(readlink -f "${EBASH_HOME}")
EBASH=$(readlink -f "${EBASH}")

# Save off the TOPDIR so we can easily find it even when etest is changing directories
TOPDIR=${PWD}

#-------------------------------------------------------------------------------------------------------------------------
# GLOBAL SETUP
#-------------------------------------------------------------------------------------------------------------------------

$(opt_parse \
    "+break   b=${BREAK:-0}    | Stop immediately on first failure." \
    "+clean   c=0              | Clean only and then exit." \
    ":debug   D=${EDEBUG:-}    | EDEBUG output." \
    "+delete  d=1              | Delete all output files when tests complete." \
    ":exclude x                | Tests whose name or file match this (bash-style) regular expression will not be run." \
    ":failures=${FAILURES:-0}  | Number of failures per-test to permit. Normally etest will return non-zero if any
                                 test fails at all. However, in certain circumstances where flaky tests exist it may
                                 be desireable to allow each test to retried a specified number of times and only
                                 classify it as a failure if that test fails more than the requested threshold." \
    ":filter  f                | Tests whose name or file match this (bash-style) regular expression will be run." \
    "+html    h=0              | Produce an HTML logfile and strip color codes out of etest.log." \
    ":log_dir                  | Directory to place logs in. Defaults to the current directory." \
    "+mount_ns=1               | Run tests inside a mount namespace." \
    ":repeat  r=${REPEAT:-1}   | Number of times to repeat each test." \
    "+summary s=0              | Display final summary to terminal in addition to logging it to etest.json." \
    "&test_list l              | File that contains a list of tests to run. This file may contain comments on lines
                                 that begin with the # character. All other nonblank lines will be interpreted as
                                 things that could be passed as @tests -- directories, executable scripts, or .etest
                                 files. Relative paths will be interpreted against the current directory. This option
                                 may be specified multiple times." \
    "+verbose v=${VERBOSE:-0}  | Verbose output." \
    ":work_dir                 | Temporary location where etest can place temporary files. This location will be both
                                 created and deleted by etest." \
    "@tests                    | Any number of individual tests, which may be executables to be executed and checked for
                                 exit code or may be files whose names end in .etest, in which case they will be sourced
                                 and any test functions found will be executed. You may also specify directories in
                                 which case etest will recursively find executables and .etest files and treat them in
                                 similar fashion.")

# Default settings for filter an exclude. Note that we can't use the opt_parse "default value" feature for this,
# because our values are regular expressions. That is, they're fairly likely to contain pipe characters and that would
# cause opt_parse to do crazy things.
: ${filter:=${FILTER:-}}
: ${exclude:=${EXCLUDE:-}}

# Use mount namespaces as long as:
#   1) they weren't forcibly turned off
#   2) we're on linux
#   3) we're not inside docker (because docker requires us to be privileged, and because the benefit is no longer there
#      -- docker already protects us inside a mount namespace)
if [[ ${mount_ns} -eq 1 ]] && os linux && ! running_in_docker; then
    reexec --mount-ns
fi

declare -A _EBASH_CONF
if [[ -f .ebash ]] ; then
    conf_read _EBASH_CONF .ebash
fi

START_TIME=$SECONDS

# Default log directory from conf file if unspecified on the command line
: ${log_dir:=$(conf_get _EBASH_CONF etest.log_dir)}
: ${log_dir:=.}
log_dir=$(readlink -f ${log_dir})

# Default set of tests from conf file if unspecified on the command line
if array_empty tests && array_empty test_list ; then
    tests=( $(conf_get _EBASH_CONF etest.tests) )
fi

if array_not_empty test_list ; then
    # Read non-comment lines from test_list and treat them as if they were passed as arguments to this script
    edebug "Grabbing test list from $(lval test_list)"
    array_init_nl tests_from_list "$(grep -vP '^\s*(#.*)$' "${test_list[@]}")"
    if array_not_empty tests_from_list ; then
        edebug "Found $(lval test_list tests_from_list)"
        tests+=( "${tests_from_list[@]}" )
    else
        edebug "Found no tests in $(lval test_list)"
    fi
fi

# Default working directory from conf file if unspecified, or ./output if not in either place
: ${work_dir:=$(conf_get _EBASH_CONF etest.work_dir)}
: ${work_dir:=./output}
work_dir=$(readlink -f ${work_dir})

EDEBUG=${debug}

(( ${repeat} < 1 )) && repeat=1
[[ ${EDEBUG:-0} != "0" ]] && verbose=1 || true
edebug "$(lval TOPDIR TEST_DIR) $(opt_dump)"

if ! cgroup_supported ; then
    export ETEST_CGROUP_BASE=unsupported
else
    # Global cgroup name for all unit tests run here
    export ETEST_CGROUP_BASE="etest"
fi
export ETEST_CGROUP="${ETEST_CGROUP_BASE}/$$"

# Setup logfile
exec {ETEST_STDERR_FD}<&2
ETEST_LOG=${log_dir}/etest.log
ETEST_JSON=${log_dir}/etest.json

elogrotate "${ETEST_JSON}"
elogfile --rotate_count=10 --tail=${verbose} ${ETEST_LOG}

# Setup redirection for "etest" and actual "test" output
if [[ ${verbose} -eq 0 ]]; then
    ETEST_OUT="$(fd_path)/${ETEST_STDERR_FD}"
    TEST_OUT="/dev/null"
else
    ETEST_OUT="/dev/null"
    TEST_OUT="/dev/stderr"
fi

#-------------------------------------------------------------------------------------------------------------------------
# TEST UTILITY FUNCTIONS
#-------------------------------------------------------------------------------------------------------------------------

die_handler()
{
    $(opt_parse \
        ":rc return_code r=1 | Return code that die will exit with")

    # Append any error message to logfile
    if [[ ${verbose} -eq 0 ]]; then
        echo "" >&2
        eerror "${@}"

        # Call eerror_stacktrace but skip top three frames to skip over the frames
        # containing stacktrace_array, eerror_stacktrace and die itself. Also skip
        # over the initial error message since we already displayed it.
        eerror_stacktrace -f=4 -s

    fi &>${ETEST_OUT}

    exit ${rc}
}

# Returns success if there are no stale processes remaining in the cgroup for
# this test and failure if there are any.
#
no_process_leaks_remain()
{
    if ! cgroup_supported; then
        return 0
    fi

    # If the cgroup no longer exists, we're in good shape because you can't
    # destroy a cgroup until all its processes are dead.
    $(tryrc -r=exists_rc cgroup_exists ${ETEST_CGROUP})
    if [[ ${exists_rc} -ne 0 ]] ; then
        return 0
    fi

    # As long as it existed just now, we can assume cgroup_pids will exist,
    # because nothing else will destroy the cgroup except for us.
    local remaining_pids=""
    remaining_pids=$(cgroup_pids ${ETEST_CGROUP})
    etestmsg "$(lval remaining_pids exists_rc ETEST_CGROUP)"
    [[ -z ${remaining_pids} ]]
}

assert_no_process_leaks()
{
    # Error stacks generated here should produce output, even though etest has
    # them wrapped in a try.
    __EBASH_INSIDE_TRY=0

    edebug "Waiting..."

    # Wait for up to 5 seconds for leaked processes to die off. If anything
    # lasts beyond that, we'll call it a test failure.
    $(tryrc eretry -T=5s no_process_leaks_remain)

    # The above command could have timed out but that doesn't necessarily mean
    # there are leaked processes. So KILL anything that's left, but only DIE
    # if there were actually processes leaked.
    if [[ ${rc} -ne 0 ]] && cgroup_supported ; then
        local leaked_processes=""
        leaked_processes=$(cgroup_ps ${ETEST_CGROUP})
        if [[ -n ${leaked_processes} ]]; then
            cgroup_kill_and_wait -s=SIGKILL ${ETEST_CGROUP}

            die "Leaked processes in ${ETEST_CGROUP}:\n${leaked_processes}"
        fi
    fi

    edebug "Finished"
}

assert_no_mount_leaks()
{
    $(opt_parse path)
    edebug "Checking for stale mounts under $(lval path)"

    local mounts=()
    mounts=( $(efindmnt "${path}" ) )

    if ! array_empty mounts; then
        eunmount -a -r -d=${delete} "${path}"
        eerror "Leaked under $(lval mounts path)"$'\n'"$(array_join_nl mounts)"
        die "Leaked mounts"
    fi

    if [[ ${delete} -eq 1 ]]; then
        rm --recursive --force "${path}"
    fi

    edebug "Finished"
}

global_setup()
{
    edebug "Running global_setup"

    # Create a specific directory to run this test in. That way the test can create whatever directories and files it
    # needs and assuming the test succeeds we'll auto remove the directory after the test completes.
    efreshdir "${work_dir}"

    if cgroup_supported ; then
        # And a cgroup that will contain all output
        cgroup_create ${ETEST_CGROUP}
        cgroup_move ${ETEST_CGROUP_BASE} $$
    fi

    edebug "Finished global_setup"
    return 0
}

global_teardown()
{
    if [[ ${delete} -eq 0 ]]; then
        edebug "Skipping global_teardown"
        return 0
    fi

    edebug "Running global_teardown: PID=$$ BASHPID=${BASHPID} PPID=${PPID}"

    assert_no_process_leaks
    assert_no_mount_leaks ${work_dir}

    if cgroup_supported ; then
        cgroup_destroy -r ${ETEST_CGROUP}
    fi

    # Convert logfile to HTML if requested
    if [[ ${html} -eq 1 ]] && which ansi2html &>/dev/null; then
        edebug "Converting ${ETEST_LOG} into HTML"
        cat ${ETEST_LOG} | ansi2html --scheme=xterm > ${ETEST_LOG/.log/.html}
        noansi ${ETEST_LOG}
    fi

    edebug "Finished global_teardown"
    return 0
}

run_single_test()
{
    $(opt_parse \
        ":testidx       | Test index of current test. Mose useful if the test is a function inside a test suite." \
        ":testidx_total | Total number of tests. Most useful if the test is a function inside a test suite." \
        ":work_dir      | Temporary directory that the test should use as its current working directory." \
        ":source        | Name file to be sourced in the shell that will run the test. Most useful if the test is a function
                          inside that file." \
        "testname       | Command to execute to run the test")

    local rc=0

    # If the test name or optional source name doesn't match a specified FILTER, we're done
    if ! [[ -z ${filter} || ${testname} =~ ${filter} || ( -n ${source} && ${source} =~ ${filter} ) ]] ; then
        return 0
    fi

    # If the test name does match a specified EXCLUDE, we're done
    if [[ -n ${exclude} && ${testname} =~ ${exclude} ]] ; then
        return 0
    fi

    local display_testname=${testname}
    if [[ -n ${source} ]] ; then
        display_testname="${source}:${testname}"
    fi

    local index_string="${testidx}/${testidx_total}"
    ebanner --uppercase "${testname}" INDEX=index_string REPEAT=REPEAT_STRING failures

    # If this file is being sourced then it's an ETEST so log it as a subtest
    # via einfos. Otherwise log via einfo as a top-level test script.
    if [[ -n "${source}" ]]; then
        einfos "${testname}"
    else
        EMSG_PREFIX="" einfo "${testname}"
    fi &>${ETEST_OUT}

    (( NUM_TESTS_EXECUTED += 1 ))

    local tries=0
    for (( tries=0; tries <= ${failures}; tries++ )); do

        rc=0

        # We want to make sure that any traps from the tests
        # execute _before_ we run teardown, and also we don't
        # want the teardown to run inside the test-specific
        # cgroup. This subshell solves both issues.
        try
        {
            export EBASH EBASH_HOME TEST_DIR_OUTPUT=${work_dir}
            if [[ -n ${source} ]] ; then
                source "${source}"
            fi

            # Pretend that the test _not_ executing inside a try/catch so that the
            # error stack will get printed if part of the test fails, as if etest
            # weren't running it inside a try/catch
            __EBASH_INSIDE_TRY=0

            # Create our temporary workspace in the directory specified by the caller
            efreshdir "${work_dir}"
            mkdir "${work_dir}/tmp"
            TMPDIR="$(readlink -m ${work_dir}/tmp)"
            export TMPDIR

            if cgroup_supported ; then
                cgroup_create ${ETEST_CGROUP}
                cgroup_move ${ETEST_CGROUP} ${BASHPID}
            fi

            local command="${testname}"
            if ! is_function "${testname}" ; then
                command=$(readlink -f "${testname}")
            fi

            cd "${work_dir}"

            # Run suite setup function if provided and we're on the first test
            if [[ "${testidx}" -eq 0 ]] && is_function suite_setup; then
                etestmsg "Running suite_setup $(lval testidx testidx_total)"
                suite_setup
            fi

            # Run optional test setup function if provided
            if is_function setup ; then
                etestmsg "Running setup"
                setup
            fi

            etestmsg "Running $(lval command tries failures testidx testidx_total)"
            "${command}"

            # Run optional test teardown function if provided
            if is_function teardown ; then
                etestmsg "Running teardown"
                teardown
            fi

            # Run suite teardown function if provided and we're on the last test
            if [[ "${testidx}" -eq "${testidx_total}" ]] && is_function suite_teardown; then
                etestmsg "Running suite_teardown $(lval testidx testidx_total)"
                suite_teardown
            fi
        }
        catch
        {
            rc=$?
        }
        edebug "Finished $(lval testname display_testname rc tries FAILURES)"

        if [[ ${rc} -eq 0 ]]; then
            break
        fi
    done

    local process_leak_rc=0
    if cgroup_supported ; then
        $(tryrc -r=process_leak_rc assert_no_process_leaks)
    fi

    local mount_leak_rc=0
    $(tryrc -r=mount_leak_rc assert_no_mount_leaks "${work_dir}")

    # If the test eventually passed (rc==0) but we had to try more than one time (tries > 0) then by definition
    # this is a flaky test.
    if [[ ${rc} -eq 0 && ${tries} -gt 0 ]]; then
        TESTS_FLAKY+=( "${display_testname}" )
    fi

    if [[ ${rc} -eq 0 && ${process_leak_rc} -eq 0 && ${mount_leak_rc} -eq 0 ]]; then
        einfo "$(ecolor green)${display_testname} PASSED."
        TESTS_PASSED+=( "${display_testname}" )

    elif [[ ${rc} -eq 0 && ${process_leak_rc} -ne 0 ]] ; then
        eerror "${display_testname} FAILED due to process leak."
        TESTS_FAILED+=( "${display_testname}" )
        rc=1

    elif [[ ${rc} -eq 0 && ${mount_leak_rc} -ne 0 ]] ; then
        eerror "${display_testname} FAILED due to mount leak."
        TESTS_FAILED+=( "${display_testname}" )
        rc=1

    else
        eerror "${display_testname} FAILED."
        TESTS_FAILED+=( "${display_testname}" )
    fi

    eend ${rc} &>${ETEST_OUT}

    # Unit test provided teardown
    if declare -f teardown &>/dev/null ; then
        etestmsg "Calling test_teardown"
        $(tryrc -r=teardown_rc teardown)
    fi

    # NOTE: Don't return rc here. We've already set things up so etest knows if there was a failure.
    if [[ ${break} -eq 1 && ${#TESTS_FAILED[@]} -gt 0 ]] ; then
        die "${display_testname} failed and break=1" &>${ETEST_OUT}
    fi
}

run_etest_file()
{
    $(opt_parse testfile)
    local testfilename
    testfilename=$(basename ${testfile})

    # If the test file name does match a specified exclude, we're done
    if [[ -n ${exclude} && ${testfile} =~ ${exclude} ]] ; then
        return 0
    fi

    # Get all function names that begin with ETEST_
    edebug $(lval testfile ETEST_FUNCTIONS)
    ETEST_FUNCTIONS=( $(source "${testfile}" ; declare -F | awk '$3 ~ "^ETEST" {print $3}' || true) )

    if [[ ${#ETEST_FUNCTIONS[@]} -eq 0 ]] ; then
        ewarn "No tests found in $(lval testfile)"
        return 0
    fi

    EMSG_PREFIX="" einfo "${testfilename}" &>${ETEST_OUT}

    local idx
    for idx in $(array_indexes ETEST_FUNCTIONS); do

        local testfunc=${ETEST_FUNCTIONS[$idx]}
        local test_work_dir="${work_dir}/${testfilename}/${testfunc}"

        run_single_test \
            --testidx ${idx} \
            --testidx-total $(( ${#ETEST_FUNCTIONS[@]} - 1 )) \
            --work-dir "${test_work_dir}" \
            --source "${testfile}" \
            "${testfunc}"

        if [[ ${break} -eq 1 && ${#TESTS_FAILED[@]} -gt 0 ]] ; then
            die "${testfunc} failed and break=1" &>${ETEST_OUT}
        fi

    done
}

#-------------------------------------------------------------------------------------------------------------------------
# GLOBAL SETUP
#-------------------------------------------------------------------------------------------------------------------------

global_setup
trap_add global_teardown

# If clean only is requested exit immediately. The "clean" is done via global_setup and global_teardown.
[[ ${clean} -eq 1 ]] && exit 0

#-------------------------------------------------------------------------------------------------------------------------
# MAIN
#-------------------------------------------------------------------------------------------------------------------------

declare -ag TESTS_PASSED=()
declare -ag TESTS_FAILED=()
declare -ag TESTS_FLAKY=()
declare -g  NUM_TESTS_EXECUTED=0

# Expand tests to find all standalone executable scripts as well as any *.etest files.
TESTS=(
    $(find "${tests[@]}" -type f -executable     | sort || true)
    $(find "${tests[@]}" -type f -name "*.etest" | sort || true)
)

OS="$(os_pretty_name)"
if [[ "${verbose}" -eq 1 ]]; then
    ebanner --uppercase "ETEST" OS break exclude failures filter repeat
else
    ebanner --uppercase "ETEST" OS break exclude failures filter repeat &>${ETEST_OUT}
fi

for (( ITERATION=1; ITERATION<=${repeat}; ITERATION++ )); do
    REPEAT_STRING="${ITERATION}/${repeat}"

    for filename in "${TESTS[@]}"; do
        if [[ "${filename}" =~ \.etest$ ]]; then
            run_etest_file "${filename}"
        else
            run_single_test --work-dir "${work_dir}/standalone" "${filename}"
        fi
    done
done

vcs_pack=""
if [[ -d ".hg" ]] && command_exists hg; then
    pack_set vcs_pack \
        type="hg"                                       \
        info="$(hg id --id)"                            \
        url="$(hg paths default)"                       \
        branch="$(hg branch)"                           \
        bookmark="$(hg book | awk '/ * / {print $2}')"  \
        commit="$(hg id --id)"

elif [[ -d ".git" ]] && command_exists git && git rev-parse --is-inside-work-tree &>/dev/null; then
    pack_set vcs_pack \
        type="git"                                                \
        info="$(git describe --abbrev=7 --always --tags --dirty)" \
        url="$(git config --get remote.origin.url)"               \
        branch="$(git rev-parse --abbrev-ref HEAD)"               \
        bookmark=""                                               \
        commit="$(git rev-parse --short=12 HEAD)"
fi

RUNTIME=$(( SECONDS - START_TIME ))

{
    echo
    message="Finished testing $(pack_get vcs_pack info)."
    message+=" $(( ${#TESTS_PASSED[@]} ))/${NUM_TESTS_EXECUTED} tests passed"
    message+=" in ${RUNTIME} seconds."

    if [[ ${#TESTS_FAILED[@]} -gt 0 ]]; then
        eerror "${message}"
    else
        einfo "${message}"
    fi
    echo

    if array_not_empty TESTS_FAILED; then
        eerror "FAILED TESTS:"
        for failed_test in "${TESTS_FAILED[@]}" ; do
            echo "$(ecolor "red")      ${failed_test}" >&2
        done
        ecolor off >&2
    fi

    if array_not_empty TESTS_FLAKY; then
        ewarn "FLAKY TESTS:"
        for flaky_test in "${TESTS_FLAKY[@]}" ; do
            echo "$(ecolor "yellow")      ${flaky_test}" >&2
        done
        ecolor off >&2
    fi

# Create a summary file with relevant statistics
if command_exists jq; then

jq . << EOF > ${ETEST_JSON}
{
    "numTestsExecuted": "${NUM_TESTS_EXECUTED}",
    "numTestsPassed": "${#TESTS_PASSED[@]}",
    "numTestsFailed": "${#TESTS_FAILED[@]}",
    "numTestsFlaky": "${#TESTS_FLAKY[@]}",
    "testsPassed": $(array_to_json TESTS_PASSED),
    "testsFailed": $(array_to_json TESTS_FAILED),
    "testsFlaky": $(array_to_json TESTS_FLAKY),
    "runtime": "${RUNTIME} seconds",
    "datetime": "$(etimestamp_rfc3339)",
    "options": {
        "break": "${break}",
        "clean": "${clean}",
        "debug": "${debug}",
        "delete": "${delete}",
        "exclude": "${exclude}",
        "failures": "${failures}",
        "filter": "${filter}",
        "html": "${html}",
        "log_dir": "${log_dir}",
        "mount_ns": "${mount_ns}",
        "repeat": "${repeat}",
        "test_list": $(array_to_json test_list),
        "tests": $(array_to_json tests),
        "verbose": "${verbose}"
    },
    "vcs": $(pack_to_json vcs_pack)
}
EOF

    # Additionally display summary output to the terminal if requested
    if [[ "${summary}" -eq 1 ]]; then
        einfo "Summary"
        jq --color-output . ${ETEST_JSON}
    fi
fi

} |& tee -a ${ETEST_LOG} >&${ETEST_STDERR_FD}

exit ${#TESTS_FAILED[@]}
