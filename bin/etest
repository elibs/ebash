#!/usr/bin/env bash
#
# Copyright 2011-2018, Marshall McMullen <marshall.mcmullen@gmail.com>
# Copyright 2011-2018, SolidFire, Inc. All rights reserved.
#
# This program is free software: you can redistribute it and/or modify it under the terms of the Apache License
# as published by the Apache Software Foundation, either version 2 of the License, or (at your option) any later
# version.

: ${EMSG_PREFIX:=time}

: ${EBASH_HOME:=$(dirname $0)/..}
: ${EBASH:=${EBASH_HOME}/share}
source ${EBASH}/ebash.sh || { echo "Unable to source ebash." ; exit 1 ; }
export EBASH

if [[ ${__EBASH_OS} == "Linux" ]] ; then
    reexec --sudo
fi

# Normalize EBASH path in case any tests depend on it looking... normal.
# Note: wait until after sourcing so that we can let ebash make sure we get
# GNU readlink rater than BSD readlink.
EBASH_HOME=$(readlink -f "${EBASH_HOME}")
EBASH=$(readlink -f "${EBASH}")

#-------------------------------------------------------------------------------------------------------------------------
# GLOBAL SETUP
#-------------------------------------------------------------------------------------------------------------------------

$(opt_parse \
    "+break   b=${BREAK:-0}    | Stop immediately on first failure." \
    "+clean   c=0              | Clean only and then exit." \
    ":debug   D=${EDEBUG:-}    | EDEBUG output." \
    "+delete  d=1              | Delete all output files when tests complete." \
    ":exclude x                | Tests whose name or file match this (bash-style) regular expression will not be run." \
    ":failures=${FAILURES:-0}  | Number of failures per-test to permit. Normally etest will return non-zero if any
                                 test fails at all. However, in certain circumstances where flaky tests exist it may
                                 be desireable to allow each test to retried a specified number of times and only
                                 classify it as a failure if that test fails more than the requested threshold." \
    ":filter  f                | Tests whose name or file match this (bash-style) regular expression will be run." \
    "+html    h=0              | Produce an HTML logfile and strip color codes out of etest.log." \
    "+logfile=1                | Whether we should use elogfile to log all output." \
    ":log_dir                  | Directory to place logs in.  Defaults to the current directory." \
    "+mount_ns=1               | Run tests inside a mount namespace." \
    ":repeat  r=${REPEAT:-1}   | Number of times to repeat each test." \
    ":shard_index=0            | Shard index of tests to run. This allows splitting up our tests into chunks to help
                                 splitting our large tests into smaller chunks and run them concurrently as
                                 separate jobs." \
    ":total_shards=1           | Total number of test shards to run. By default all run in a single shard." \
    "+summary s=0              | Display final summary to terminal in addition to logging it to etest.json." \
    "&test_list l              | File that contains a list of tests to run.  This file may contain comments on lines
                                 that begin with the # character.  All other nonblank lines will be interpreted as
                                 things that could be passed as @tests -- directories, executable scripts, or .etest
                                 files.  Relative paths will be interpreted against the current directory.  This option
                                 may be specified multiple times." \
    "+verbose v=${VERBOSE:-0}  | Verbose output." \
    ":work_dir                 | Temporary location where etest can place temporary files.  This location will be both
                                 created and deleted by etest." \
    "+coverage                 | Run etest through kcov coverage tool to identify and report code coverage for all
                                 executed code." \
    ":coverage_dir=coverage    | Directory to store coverage output in." \
    ":coverage_publish         | Upstream coverage analysis provider to publish results to (e.g. codecov.io)" \
    "@tests                    | Any number of individual tests, which may be executables to be executed and checked for
                                 exit code or may be files whose names end in .etest, in which case they will be sourced
                                 and any test functions found will be executed.  You may also specify directories in
                                 which case etest will recursively find executables and .etest files and treat them in
                                 similar fashion.")

# Default settings for filter an exclude.  Note that we can't use the opt_parse "default value" feature for this,
# because our values are regular expressions.  That is, they're fairly likely to contain pipe characters and that would
# cause opt_parse to do crazy things.
: ${filter:=${FILTER:-}}
: ${exclude:=${EXCLUDE:-}}

# Use mount namespaces as long as:
#   1) they weren't forcibly turned off
#   2) we're on linux
#   3) we're not inside docker (because docker requires us to be privileged, and because the benefit is no longer there
#      -- docker already protects us inside a mount namespace)
if [[ ${mount_ns} -eq 1 ]] && os linux && ! running_in_docker; then
    reexec --mount-ns
fi

declare -A _EBASH_CONF
if [[ -r .ebash ]] ; then
    conf_read _EBASH_CONF .ebash
fi

START_TIME=$SECONDS

# Default log directory from conf file if unspecified on the command line
: ${log_dir:=$(conf_get _EBASH_CONF etest.log_dir)}
: ${log_dir:=.}
log_dir=$(readlink -f ${log_dir})

# Default set of tests from conf file if unspecified on the command line
if array_empty tests && array_empty test_list ; then
    tests=( $(conf_get _EBASH_CONF etest.tests) )
fi

if array_not_empty test_list ; then
    # Read non-comment lines from test_list and treat them as if they were passed as arguments to this script
    edebug "Grabbing test list from $(lval test_list)"
    array_init_nl tests_from_list "$(grep -vP '^\s*(#.*)$' "${test_list[@]}")"
    if array_not_empty tests_from_list ; then
        edebug "Found $(lval test_list tests_from_list)"
        tests+=( "${tests_from_list[@]}" )
    else
        edebug "Found no tests in $(lval test_list)"
    fi
fi

# Default working directory from conf file if unspecified, or ./output if not in either place
: ${work_dir:=$(conf_get _EBASH_CONF etest.work_dir)}
: ${work_dir:=./output}
work_dir=$(readlink -f ${work_dir})

EDEBUG=${debug}

(( ${repeat} < 1 )) && repeat=1
[[ ${EDEBUG:-0} != "0" ]] && verbose=1 || true
edebug "$(lval TEST_DIR) $(opt_dump)"

if ! cgroup_supported ; then
    export ETEST_CGROUP_BASE=unsupported
else
    # Global cgroup name for all unit tests run here
    export ETEST_CGROUP_BASE="etest"
fi
export ETEST_CGROUP="${ETEST_CGROUP_BASE}/$$"

# Setup logfile
exec {ETEST_STDERR_FD}<&2
ETEST_LOG=${log_dir}/etest.log
ETEST_JSON=${log_dir}/etest.json

# Setup redirection for "etest" and actual "test" output
if [[ ${logfile} -eq 0 && ${verbose} -eq 0 ]]; then

    # If we can't use elogfile and we don't want verbose output we are forced to do some trickery to get the desired
    # result. Yes this is a bit convoluted since we need to basically show the terse output on the console but also
    # send a copy of both stdout and stderr to the log file. So basically we send stdout to the logfile, then we
    # create a new file descriptor (3) and associate that with what is our current stderr. Then we can send stderr to
    # where stdout has been redirect so that they both end up in the logfile.
    exec 1>${ETEST_LOG}
    exec 3>&2
    exec 2>&1
    exec {ETEST_OUT_FD}<&3
    ETEST_OUT="$(fd_path)/${ETEST_OUT_FD}"
elif [[ ${verbose} -eq 0 ]]; then
    ETEST_OUT="$(fd_path)/${ETEST_STDERR_FD}"
else
    ETEST_OUT="/dev/null"
fi

# Coverage mode
if [[ ${coverage} -eq 1 ]]; then
    command_exists kcov

    efreshdir "${coverage_dir}"

    export COLUMNS
    kcov --bash-method=DEBUG "${coverage_dir}" \
        "$0" "${__EBASH_FULL_ARGS[@]}" --no-coverage --no-logfile

    einfo "Coverage data: file://$(readlink -m ${coverage_dir}/index.html)"

    # Optionally publish the coverage data
    if [[ -n "${coverage_publish}" ]]; then

        einfo "Publishing code coverage to ${coverage_publish}"

        case "${coverage_publish}" in
            "codecov.io")
                bash <(curl -s https://codecov.io/bash)
                ;;
            *)
                die "Unsupported $(lval coverage_publish)"
        esac
    fi

    exit 0
fi

if [[ ${logfile} -eq 1 ]]; then
    elogrotate "${ETEST_JSON}"
    elogfile --rotate_count=10 --tail=${verbose} ${ETEST_LOG}
fi

#-------------------------------------------------------------------------------------------------------------------------
# TEST UTILITY FUNCTIONS
#-------------------------------------------------------------------------------------------------------------------------

die_handler()
{
    $(opt_parse \
        ":rc return_code r=1 | Return code that die will exit with")

    # Append any error message to logfile
    if [[ ${verbose} -eq 0 ]]; then
        echo "" >&2
        eerror "${@}"

        # Call eerror_stacktrace but skip top three frames to skip over the frames
        # containing stacktrace_array, eerror_stacktrace and die itself. Also skip
        # over the initial error message since we already displayed it.
        eerror_stacktrace -f=4 -s

    fi &>${ETEST_OUT}

    exit ${rc}
}

# Returns success if there are no stale processes remaining in the cgroup for
# this test and failure if there are any.
#
no_process_leaks_remain()
{
    if ! cgroup_supported; then
        return 0
    fi

    # If the cgroup no longer exists, we're in good shape because you can't
    # destroy a cgroup until all its processes are dead.
    $(tryrc -r=exists_rc cgroup_exists ${ETEST_CGROUP})
    if [[ ${exists_rc} -ne 0 ]] ; then
        return 0
    fi

    # As long as it existed just now, we can assume cgroup_pids will exist,
    # because nothing else will destroy the cgroup except for us.
    local remaining_pids=""
    remaining_pids=$(cgroup_pids ${ETEST_CGROUP})
    etestmsg "$(lval remaining_pids exists_rc ETEST_CGROUP)"
    [[ -z ${remaining_pids} ]]
}

assert_no_process_leaks()
{
    # Error stacks generated here should produce output, even though etest has
    # them wrapped in a try.
    __EBASH_INSIDE_TRY=0

    edebug "Waiting..."

    # Wait for up to 5 seconds for leaked processes to die off.  If anything
    # lasts beyond that, we'll call it a test failure.
    $(tryrc eretry -T=5s no_process_leaks_remain)

    # The above command could have timed out but that doesn't necessarily mean
    # there are leaked processes. So KILL anything that's left, but only DIE
    # if there were actually processes leaked.
    if [[ ${rc} -ne 0 ]] && cgroup_supported ; then
        local leaked_processes=""
        leaked_processes=$(cgroup_ps ${ETEST_CGROUP})
        if [[ -n ${leaked_processes} ]]; then
            cgroup_kill_and_wait -s=SIGKILL ${ETEST_CGROUP}

            die "Leaked processes in ${ETEST_CGROUP}:\n${leaked_processes}"
        fi
    fi

    edebug "Finished"
}

assert_no_mount_leaks()
{
    $(opt_parse path)
    edebug "Checking for stale mounts under $(lval path)"

    local mounts=()
    mounts=( $(efindmnt "${path}" ) )

    if ! array_empty mounts; then
        eunmount -a -r -d=${delete} "${path}"
        eerror "Leaked under $(lval mounts path)"$'\n'"$(array_join_nl mounts)"
        die "Leaked mounts"
    fi

    if [[ ${delete} -eq 1 ]]; then
        rm --recursive --force "${path}"
    fi

    edebug "Finished"
}

global_setup()
{
    edebug "Running global_setup"

    # Create a specific directory to run this test in. That way the test can create whatever directories and files it
    # needs and assuming the test succeeds we'll auto remove the directory after the test completes.
    efreshdir "${work_dir}"

    if cgroup_supported ; then
        # And a cgroup that will contain all output
        cgroup_create ${ETEST_CGROUP}
        cgroup_move ${ETEST_CGROUP_BASE} $$
    fi

    edebug "Finished global_setup"
    return 0
}

global_teardown()
{
    if [[ ${delete} -eq 0 ]]; then
        edebug "Skipping global_teardown"
        return 0
    fi

    edebug "Running global_teardown: PID=$$ BASHPID=${BASHPID} PPID=${PPID}"

    assert_no_process_leaks
    assert_no_mount_leaks ${work_dir}

    if cgroup_supported ; then
        cgroup_destroy -r ${ETEST_CGROUP}
    fi

    # Convert logfile to HTML if requested
    if [[ ${html} -eq 1 ]] && which ansi2html &>/dev/null; then
        edebug "Converting ${ETEST_LOG} into HTML"
        cat ${ETEST_LOG} | ansi2html --scheme=xterm > ${ETEST_LOG/.log/.html}
        noansi ${ETEST_LOG}
    fi

    edebug "Finished global_teardown"
    return 0
}

run_single_test()
{
    $(opt_parse \
        ":work_dir | Temporary directory that the test should use as its current working directory." \
        ":source   | Name file to be sourced in the shell that will run the test.  Most useful if the test is a function
                     inside that file." \
        "testname  | Command to execute to run the test")

    local rc=0

    # If the test name or optional source name doesn't match a specified FILTER, we're done
    if ! [[ -z ${filter} || ${testname} =~ ${filter} || ( -n ${source} && ${source} =~ ${filter} ) ]] ; then
        return 0
    fi

    # If the test name does match a specified EXCLUDE, we're done
    if [[ -n ${exclude} && ${testname} =~ ${exclude} ]] ; then
        return 0
    fi

    local display_testname=${testname}
    if [[ -n ${source} ]] ; then
        display_testname="${source}:${testname}"
    fi

    ebanner --uppercase "${display_testname}" REPEAT=REPEAT_STRING failures

    einfos ${display_testname} &>${ETEST_OUT}

    (( NUM_TESTS_EXECUTED += 1 ))

    local tries=0
    for (( tries=0; tries <= ${failures}; tries++ )); do

        rc=0

        # We want to make sure that any traps from the tests
        # execute _before_ we run teardown, and also we don't
        # want the teardown to run inside the test-specific
        # cgroup.  This subshell solves both issues.
        try
        {
            export EBASH EBASH_HOME TEST_DIR_OUTPUT=${work_dir}
            if [[ -n ${source} ]] ; then
                source "${source}"
            fi

            # Pretend that the test _not_ executing inside a try/catch so that the
            # error stack will get printed if part of the test fails, as if etest
            # weren't running it inside a try/catch
            __EBASH_INSIDE_TRY=0

            # Create our temporary workspace in the directory specified by the caller
            efreshdir "${work_dir}"
            mkdir "${work_dir}/tmp"
            TMPDIR="$(readlink -m ${work_dir}/tmp)"
            export TMPDIR

            if cgroup_supported ; then
                cgroup_create ${ETEST_CGROUP}
                cgroup_move ${ETEST_CGROUP} ${BASHPID}
            fi

            # Unit test provided setup
            if is_function setup ; then
                etestmsg "Calling setup"
                setup
            fi

            local command="${testname}"
            if ! is_function "${testname}" ; then
                command=$(readlink -f "${testname}")
            fi

            cd "${work_dir}"

            etestmsg "Calling test $(lval tries failures)"
            "${command}"

            if is_function teardown ; then
                etestmsg "Calling teardown"
                teardown
            fi
        }
        catch
        {
            rc=$?
        }
        edebug "Finished $(lval testname display_testname rc tries FAILURES)"

        if [[ ${rc} -eq 0 ]]; then
            break
        fi
    done

    local process_leak_rc=0
    if cgroup_supported ; then
        $(tryrc -r=process_leak_rc assert_no_process_leaks)
    fi

    local mount_leak_rc=0
    $(tryrc -r=mount_leak_rc assert_no_mount_leaks "${work_dir}")

    # If the test eventually passed (rc==0) but we had to try more than one time (tries > 0) then by definition
    # this is a flaky test.
    if [[ ${rc} -eq 0 && ${tries} -gt 0 ]]; then
        TESTS_FLAKY+=( "${display_testname}" )
    fi

    if [[ ${rc} -eq 0 && ${process_leak_rc} -eq 0 && ${mount_leak_rc} -eq 0 ]]; then
        einfo "$(ecolor green)${display_testname} PASSED."
        TESTS_PASSED+=( "${display_testname}" )

    elif [[ ${rc} -eq 0 && ${process_leak_rc} -ne 0 ]] ; then
        eerror "${display_testname} FAILED due to process leak."
        TESTS_FAILED+=( "${display_testname}" )
        rc=1

    elif [[ ${rc} -eq 0 && ${mount_leak_rc} -ne 0 ]] ; then
        eerror "${display_testname} FAILED due to mount leak."
        TESTS_FAILED+=( "${display_testname}" )
        rc=1

    else
        eerror "${display_testname} FAILED."
        TESTS_FAILED+=( "${display_testname}" )
    fi

    eend ${rc} &>${ETEST_OUT}

    # Unit test provided teardown
    if declare -f teardown &>/dev/null ; then
        etestmsg "Calling test_teardown"
        $(tryrc -r=teardown_rc teardown)
    fi

    # NOTE: Don't return rc here.  We've already set things up so etest knows if there was a failure.
}

run_etest_file()
{
    $(opt_parse testfile)
    local testfilename
    testfilename=$(basename ${testfile})

    # If the test file name does match a specified exclude, we're done
    if [[ -n ${exclude} && ${testfile} =~ ${exclude} ]] ; then
        return 0
    fi

    # Get all function names that begin with ETEST_
    edebug $(lval testfile ETEST_FUNCTIONS)
    ETEST_FUNCTIONS=( $(source "${testfile}" ; declare -F | awk '$3 ~ "^ETEST" {print $3}' || true) )

    if [[ ${#ETEST_FUNCTIONS[@]} -eq 0 ]] ; then
        ewarn "No tests found in $(lval testfile)"
        return 0
    fi

    local testfunc
    for testfunc in ${ETEST_FUNCTIONS[@]} ; do

        local test_work_dir="${work_dir}/${testfilename}/${testfunc}"
        run_single_test --work-dir "${test_work_dir}" --source "${testfile}" "${testfunc}"

        if [[ ${break} -eq 1 && ${#TESTS_FAILED[@]} -gt 0 ]] ; then
            die "${testfunc} failed and break=1" &>${ETEST_OUT}
        fi

    done
}

#-------------------------------------------------------------------------------------------------------------------------
# GLOBAL SETUP
#-------------------------------------------------------------------------------------------------------------------------

global_setup
trap_add global_teardown

# If clean only is requested exit immediately. The "clean" is done via global_setup and global_teardown.
[[ ${clean} -eq 1 ]] && exit 0

#-------------------------------------------------------------------------------------------------------------------------
# MAIN
#-------------------------------------------------------------------------------------------------------------------------

declare -ag TESTS_PASSED=()
declare -ag TESTS_FAILED=()
declare -ag TESTS_FLAKY=()
declare -ag TESTS_SHARD=()
declare -g  NUM_TESTS_EXECUTED=0

# Expand tests to find all standalone executable scripts as well as any *.etest files.
TESTS_SHARD=(
    $(find "${tests[@]}" -type f -executable     | sort || true)
    $(find "${tests[@]}" -type f -name "*.etest" | sort || true)
)

# Split tests into shards if requested
if [[ "${total_shards}" -gt 1 ]]; then
    assert_num "${shard_index}"

    num_tests_expanded="${#TESTS_SHARD[@]}"

    # If number of shards is greater than total number of tests then the caller has made a mistake
    # in how they carved up the tests. We cannot just adjust it for them here as all the other shards
    # would also be similarly invalid. THis could lead to the same tests running multiple times in
    # each shard. So instead we just fail immediately.
    assert_le "${total_shards}" "${num_tests_expanded}" "Total shards cannot exceed number of tests."
    num_tests_per_shard=$(( ${#TESTS_SHARD[@]} / ${total_shards} ))
    assert_gt "${num_tests_per_shard}" 0
    shard_offset_start=$(( ${num_tests_per_shard} * ${shard_index} ))
    shard_offset_end=$(( ${shard_offset_start} + ${num_tests_per_shard} ))
    assert_num_le "${shard_offset_end}" "${#TESTS_SHARD[@]}"

    # Always ensure the last shard goes to the very end of the array
    if [[ $(( ${shard_index} + 1 )) -eq ${total_shards} ]]; then
        shard_offset_end="${#TESTS_SHARD[@]}"
    fi

    edebug "Sharding $(lval shard_index total_shards num_tests_expanded num_tests_per_shard shard_offset_start shard_offset_end)"
    TESTS_SHARD=("${TESTS_SHARD[@]:${shard_offset_start}:${shard_offset_end}}")
fi

OS="$(os_pretty_name)"
SHARD="($((${shard_index}+1))/${total_shards})"
if [[ "${verbose}" -eq 1 ]]; then
    ebanner --uppercase "ETEST" OS break failures filter repeat SHARD
else
    ebanner --uppercase "ETEST" OS break failures filter repeat SHARD &>${ETEST_OUT}
fi

for (( ITERATION=1; ITERATION<=${repeat}; ITERATION++ )); do
    REPEAT_STRING="(${ITERATION}/${repeat})"

    for filename in "${TESTS_SHARD[@]}"; do
        if [[ "${filename}" =~ \.etest$ ]]; then
            run_etest_file "${filename}"
        else
            run_single_test --work-dir "${work_dir}/standalone" "${filename}"
        fi
    done
done

vcs_pack=""
if [[ -d ".hg" ]] && which hg &>/dev/null ; then
    pack_set vcs_pack \
        type="hg"                                       \
        info="$(hg id --id)"                            \
        url="$(hg paths default)"                       \
        branch="$(hg branch)"                           \
        bookmark="$(hg book | awk '/ * / {print $2}')"  \
        commit="$(hg id --id)"

elif [[ -d ".git" ]] && which git &>/dev/null ; then
    pack_set vcs_pack \
        type="git"                                                \
        info="$(git describe --abbrev=7 --always --tags --dirty)" \
        url="$(git config --get remote.origin.url)"               \
        branch="$(git rev-parse --abbrev-ref HEAD)"               \
        bookmark=""                                               \
        commit="$(git rev-parse --short=12 HEAD)"
fi

RUNTIME=$(( SECONDS - START_TIME ))

{
    echo
    message="Finished testing $(pack_get vcs_pack info)."
    message+=" $(( ${#TESTS_PASSED[@]} ))/${NUM_TESTS_EXECUTED} tests passed"
    message+=" in ${RUNTIME} seconds."

    if [[ ${#TESTS_FAILED[@]} -gt 0 ]]; then
        eerror "${message}"
    else
        einfo "${message}"
    fi
    echo

    if array_not_empty TESTS_FAILED; then
        eerror "FAILED TESTS:"
        for failed_test in "${TESTS_FAILED[@]}" ; do
            echo "$(ecolor "red")      ${failed_test}" >&2
        done
        ecolor off >&2
    fi

    if array_not_empty TESTS_FLAKY; then
        ewarn "FLAKY TESTS:"
        for flaky_test in "${TESTS_FLAKY[@]}" ; do
            echo "$(ecolor "yellow")      ${flaky_test}" >&2
        done
        ecolor off >&2
    fi

# Create a summary file with relevant statistics
if command_exists jq; then

jq . << EOF > ${ETEST_JSON}
{
    "numTestsExecuted": "${NUM_TESTS_EXECUTED}",
    "numTestsPassed": "${#TESTS_PASSED[@]}",
    "numTestsFailed": "${#TESTS_FAILED[@]}",
    "numTestsFlaky": "${#TESTS_FLAKY[@]}",
    "numTestsShard": "${#TESTS_SHARD[@]}",
    "testsPassed": $(array_to_json TESTS_PASSED),
    "testsFailed": $(array_to_json TESTS_FAILED),
    "testsFlaky": $(array_to_json TESTS_FLAKY),
    "testsShard": $(array_to_json TESTS_SHARD),
    "runtime": "${RUNTIME} seconds",
    "shard": "${shard_index}/${total_shards}",
    "shardIndex": "${shard_index}",
    "totalShards": "${total_shards}",
    "datetime": "$(etimestamp_rfc3339)",
    "options": {
        "break": "${break}",
        "clean": "${clean}",
        "debug": "${debug}",
        "delete": "${delete}",
        "exclude": "${exclude}",
        "failures": "${failures}",
        "filter": "${filter}",
        "html": "${html}",
        "log_dir": "${log_dir}",
        "mount_ns": "${mount_ns}",
        "repeat": "${repeat}",
        "test_list": $(array_to_json test_list),
        "tests": $(array_to_json tests),
        "verbose": "${verbose}"
    },
    "vcs": $(pack_to_json vcs_pack)
}
EOF

    # Additionally display summary output to the terminal if requested
    if [[ "${summary}" -eq 1 ]]; then
        einfo "Summary"
        jq --color-output . ${ETEST_JSON}
    fi
fi

} |& tee -a ${ETEST_LOG} >&${ETEST_STDERR_FD}

exit ${#TESTS_FAILED[@]}
